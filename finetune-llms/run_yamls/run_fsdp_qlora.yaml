name: finetune-llama-3-fsdp-qlora
description: Finetune llama 3 with FSDP using QLoRA
tags:
  - finetune
  - llama-3
  - qlora
  - fsdp
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small
image: quay.io/vessl-ai/torch:2.3.0-cuda12.4-r4
import:
  /root/code:
    git:
      url: https://github.com/vessl-ai/examples.git
      ref: main
run:
  - command: |-
      pip install -r requirements.txt
      pip uninstall -y transformer-engine
      pip install packaging ninja einops wheel xformers
      pip install flash-attn==2.5.8 --no-build-isolation
    workdir: /root
  - command: |-
      python update_fsdp_config.py --layer-cls-name LlamaDecoderLayer
      accelerate launch --config_file configs/fsdp_config.yaml main.py \
      --model_name_or_path $MODEL_NAME \
      --dataset_name $DATASET_NAME \
      --output_dir outputs \
      --max_seq_length 2048 \
      --num_train_epochs 1 \
      --logging_steps 5 \
      --bf16 True \
      --packing True \
      --learning_rate 2e-4 \
      --lr_scheduler_type "cosine" \
      --weight_decay 1e-4 \
      --warmup_ratio 0.0 \
      --max_grad_norm 1.0 \
      --per_device_train_batch_size 8 \
      --per_device_eval_batch_size 8 \
      --gradient_accumulation_steps 1 \
      --gradient_checkpointing True \
      --use_flash_attn True \
      --upload_model True \
      --repository_name finetune-test \
      --peft_type LORA \
      --task_type CAUSAL_LM \
      --lora_r 8 \
      --lora_alpha 16 \
      --load_in_4bit True \
      --bnb_4bit_quant_type nf4 \
      --bnb_4bit_use_double_quant True \
      --bnb_4bit_compute_dtype bfloat16 \
      --bnb_4bit_quant_storage bfloat16
    workdir: /root/code
env:
  HF_TOKEN:
    secret: HF_TOKEN
  MODEL_NAME: meta-llama/Meta-Llama-3-8B-Instruct
  DATASET_NAME: kyujinpy/OpenOrca-KO
