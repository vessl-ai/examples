name: finetune-llama-3.1-fsdp-full
description: Finetune llama 3.1 with FSDP
tags:
  - finetune
  - llama-3.1
  - fsdp
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5
import:
  /root/code:
    git:
      url: https://github.com/vessl-ai/examples.git
      ref: main
run:
  - command: |-
      pip install -r requirements.txt
      pip install flash-attn==2.6.3 --no-build-isolation
      python update_fsdp_config.py --layer-cls-name LlamaDecoderLayer
      accelerate launch --config_file configs/fsdp_config.yaml main.py \
      --model_name_or_path $MODEL_NAME \
      --dataset_name $DATASET_NAME \
      --output_dir outputs \
      --max_seq_length 2048 \
      --num_train_epochs 1 \
      --logging_steps 5 \
      --bf16 True \
      --learning_rate 1e-5 \
      --weight_decay 1e-2 \
      --per_device_train_batch_size 8 \
      --per_device_eval_batch_size 8 \
      --gradient_accumulation_steps 1 \
      --gradient_checkpointing True \
      --upload_model True \
      --repository_name $REPOSITORY_NAME
    workdir: /root/code/finetune-llms
env:
  HF_TOKEN:
    secret: HF_TOKEN
  MODEL_NAME: meta-llama/Meta-Llama-3.1-8B-Instruct
  DATASET_NAME: Amod/mental_health_counseling_conversations
  REPOSITORY_NAME: Llama-3.1-8B-counselor
