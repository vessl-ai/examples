name: finetune-gpt-oss-lora
description: Finetune GPT-OSS by OpenAI
tags:
  - finetune
  - gpt-oss
  - moe
resources:
  cluster: vessl-kr-h100-80g-sxm
  preset: gpu-h100-80g-small
image: quay.io/vessl-ai/torch:2.8.0-cuda12.8
import:
  /root/code:
    git:
      url: https://github.com/vessl-ai/examples.git
      ref: main
run:
  - command: |-
      pip install -r requirements.txt
      pip install triton>=3.4.0
      python -u main.py \
      --model_name_or_path $MODEL_NAME \
      --dataset_name $DATASET_NAME \
      --output_dir gpt-oss-20b-multilingual-reasoner \
      --max_length 2048 \
      --num_train_epochs 1 \
      --logging_steps 1 \
      --bf16 True \
      --learning_rate 2e-4 \
      --warmup_ratio 0.03 \
      --lr_scheduler_type cosine_with_min_lr \
      --lr_scheduler_kwargs '{"min_lr_rate": 0.1}' \
      --per_device_train_batch_size 4 \
      --per_device_eval_batch_size 4 \
      --gradient_accumulation_steps 4 \
      --gradient_checkpointing True \
      --peft_type LORA \
      --lora_r 32 \
      --lora_alpha 64 \
      --lora_dropout 0.1 \
      --lora_target_modules all-linear \
      --load_in_4bit True \
      --bnb_4bit_compute_dtype bfloat16 \
      --bnb_4bit_quant_type nf4 \
      --upload_model True \
      --repository_name $REPOSITORY_NAME \
    workdir: /root/code/runs/finetune-llms
env:
  HF_TOKEN:
    secret: HF_TOKEN
  MODEL_NAME: openai/gpt-oss-20b  # Change to openai/gpt-oss-120b for 120B model training
  DATASET_NAME: HuggingFaceH4/Multilingual-Thinking
  REPOSITORY_NAME: gpt-oss-20b-multilingual-reasoner
  TOKENIZERS_PARALLELISM: "false"
  PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
