{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    MixtralForCausalLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "model = MixtralForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"How can I create a new model via web?\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": instruction}], return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_length=512,\n",
    "    do_sample=False,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_PATH = \"/lora/checkpoint-100\"\n",
    "QLORA_PATH = \"/qlora/checkpoint-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = inputs.shape[1]\n",
    "\n",
    "outputs = model.generate(inputs, generation_config=generation_config)\n",
    "print(\"==BASE MODEL==\")\n",
    "print(tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True))\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(model, \"LORA_PATH\")\n",
    "lora_output = lora_model.generate(inputs, generation_config=generation_config)\n",
    "print(\"==LORA MODEL==\")\n",
    "print(tokenizer.decode(lora_output[0][input_len:], skip_special_tokens=True))\n",
    "\n",
    "qlora_model = PeftModel.from_pretrained(model, \"QLORA_PATH\")\n",
    "qlora_output = qlora_model.generate(inputs, generation_config=generation_config)\n",
    "print(\"==QLORA MODEL==\")\n",
    "print(tokenizer.decode(qlora_output[0][input_len:], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
