# vessl serve revision create --serving openllm -f revision-v2.yaml

message: Extend max model length on Mistral-7B-Instruct-v0.2
image: quay.io/vessl-ai/torch:2.2.0-cuda12.3-r3
resources:
  name: gpu-a10g-small
run: pip install --upgrade openllm[vllm]; openllm start mistralai/Mistral-7B-Instruct-v0.2 --backend vllm --port 3000 --max-model-len 4096
autoscaling:
  min: 1
  max: 3
  metric: gpu
  target: 60
ports:
  - port: 3000
    name: openllm
    type: http
