name: vllm-server
description: LLM server with vLLM and Prometheus monitoring
tags:
  - vllm
  - model=mistral-7b-instruct-v0.2
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small
image: quay.io/vessl-ai/torch:2.2.0-cuda12.3-r3
import:
  /code/:
    git:
      url: github.com/vessl-ai/examples.git
      ref: main
run:
  - command: |-
      # Install Prometheus
      export PROMETHEUS_VERSION=2.49.1
      wget https://github.com/prometheus/prometheus/releases/download/v$PROMETHEUS_VERSION/prometheus-$PROMETHEUS_VERSION.linux-amd64.tar.gz
      tar -xvf prometheus-$PROMETHEUS_VERSION.linux-amd64.tar.gz
      mv prometheus-$PROMETHEUS_VERSION.linux-amd64 ./prometheus
      rm prometheus-$PROMETHEUS_VERSION.linux-amd64.tar.gz
      cp /code/vllm-online-serving/monitoring/prometheus.yml /app/prometheus/prometheus.yml
      /app/prometheus/prometheus --config.file=/app/prometheus/prometheus.yml &
    workdir: /app
  - command: |-
      # Install dependencies and patch vLLM codebase to support advanced service metrics
      # Note: This patch is not required in future versions of vLLM

      cd vllm-online-serving
      pip install -r requirements.txt
      cp monitoring/vllm-metrics-patch/engine/llm_engine.py /usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py
      cp monitoring/vllm-metrics-patch/engine/metrics.py /usr/local/lib/python3.10/dist-packages/vllm/engine/metrics.py
      cp monitoring/vllm-metrics-patch/sequence.py /usr/local/lib/python3.10/dist-packages/vllm/sequence.py
    workdir: /code
  - command: |-
      # Start vLLM API server
      cd vllm-online-serving
      python -m api --model mistralai/Mistral-7B-Instruct-v0.2 --max-model-len 8192 --disable-log-requests
    workdir: /code
ports:
  - name: vllm
    type: http
    port: 8000
  - name: prometheus
    type: http
    port: 9090
termination_protection: false
