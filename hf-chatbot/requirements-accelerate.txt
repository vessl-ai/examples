# Packages to install to enable faster inference (quantized models, FlashAttention2, etc)
autoawq==0.2.4
flash-attn==2.5.7
