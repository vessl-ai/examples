name: llama-3-1-70b-gpu-large
message: Quickstart to serve Llama 3.1 chatbot
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-large
run:
- command: |
    pip install autoawq==0.2.6
    pip install vllm==0.6.3
    pip install flash-attn==2.6.3
    vllm serve $MODEL_NAME --tensor-parallel-size $TENSOR_PARALLEL_SIZE --max-model-len $MAX_MODEL_LEN --gpu-memory-utilization $GPU_MEMORY_UTILIZATION --enforce-eager
  workdir: /root
env:
  MODEL_NAME: neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16
  TENSOR_PARALLEL_SIZE: 4
  GPU_MEMORY_UTILIZATION: 0.9
  MAX_MODEL_LEN: 65536
ports:
- port: 8000
service:
  autoscaling:
    max: 1
    metric: cpu
    min: 1
    target: 50
  expose: 8000
