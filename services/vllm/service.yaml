name: vllm-server
message: LLM server with vLLM
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small-spot
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5
run:
  - command: |-
      # Start vLLM API server
      pip install autoawq==0.2.6
      pip install vllm==0.6.0
      pip uninstall -y transformer-engine
      pip install flash-attn==2.6.3
      vllm serve $MODEL_NAME --max-model-len 65536 --disable-frontend-multiprocessing
ports:
  - name: vllm
    type: http
    port: 8000
service:
  autoscaling:
    min: 1
    max: 2
    metric: cpu
    target: 50
  expose: 8000
  monitoring:
    - port: 8000
      path: /metrics
env:
  MODEL_NAME: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
  HF_TOKEN: HF_TOKEN  # Your Huggingface API token
