name: vllm-llama-3-1-server
message: Quickstart to serve Llama 3.1 model with vllm.
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5
resources:
  cluster: vessl-gcp-oregon
  preset: gpu-l4-small-spot
run:
- command: |
    apt update && apt install -y libgl1
    pip install --upgrade vllm fastapi pydantic
    vllm serve $MODEL_NAME --max-model-len 65536
env:
  MODEL_NAME: hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
ports:
- port: 8000
service:
  autoscaling:
    max: 2
    metric: cpu
    min: 1
    target: 50
  monitoring:
    - port: 8000
      path: /metrics
  expose: 8000
